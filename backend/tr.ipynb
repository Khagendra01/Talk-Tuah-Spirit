{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'draw'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 212\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[32m    211\u001b[39m graph = workflow.compile()\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mworkflow_graph.png\u001b[39m\u001b[33m\"\u001b[39m, prog=\u001b[33m\"\u001b[39m\u001b[33mdot\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_rag_system\u001b[39m(did: \u001b[38;5;28mstr\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, past_conv: \u001b[38;5;28mstr\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    217\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03m    Query the RAG system.\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Graph' object has no attribute 'draw'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from typing import TypedDict\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Note: The following import is the corrected version for the deprecation warning,\n",
    "# though it is not used in this specific file, it's good practice to update it\n",
    "# in your project (e.g., in your 'storefigure.py' file).\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize MongoDB client\n",
    "uri = os.getenv('MONGODB_URI')\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "dbName = os.getenv('MONGODB_DB_NAME')\n",
    "collectionName = \"vecty\"\n",
    "collection = client[dbName][collectionName]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    pastcon: str\n",
    "    answer: str\n",
    "    attempts: int\n",
    "    can_answer: bool\n",
    "    doc_id: int\n",
    "    name: str\n",
    "    is_famous: bool\n",
    "\n",
    "def retrieve(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve documents from MongoDB based on the question.\n",
    "    \"\"\"\n",
    "    query_embedding = embeddings.embed_query(state[\"question\"])\n",
    "    \n",
    "    results = collection.aggregate([\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": 4,\n",
    "                \"index\": \"vector_index\",\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$match\": {\n",
    "                \"id\": state[\"doc_id\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"text\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    retrieved_docs = \"\\n\".join([d['text'] for d in results])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# RENAMED the function and node to avoid conflict with the state key\n",
    "def check_if_famous(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the person is a well-known public figure.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Is '{state['name']}' a famous person or a well-known public figure? Answer with 'Yes' or 'No'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Is {state['name']} famous?\"}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    is_famous_person = response.content.strip().lower() == \"yes\"\n",
    "    return {\"is_famous\": is_famous_person}\n",
    "\n",
    "def check_answerable(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the question can be answered from the context or past conversation.\n",
    "    \"\"\"\n",
    "    context = state[\"context\"]\n",
    "    # If the context is empty and the person isn't famous, no need to ask the LLM\n",
    "    if not context and not state.get(\"is_famous\"):\n",
    "        return {\"can_answer\": False}\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Determine if the given context or past conversation related to {state['name']} is sufficient to answer the question. Respond with 'Yes' or 'No'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {state['question']}\\nContext: {context}\\nPast Conversation: {state['pastcon']}\"}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    can_answer = response.content.strip().lower() == \"yes\"\n",
    "    return {\"can_answer\": can_answer}\n",
    "\n",
    "def generate_answer(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Generate an answer based on the context or general knowledge.\n",
    "    \"\"\"\n",
    "    context = state[\"context\"]\n",
    "    system_message = (\n",
    "        f\"Act as {state['name']}. Respond in a conversational tone, keeping your answers brief and to the point. \"\n",
    "        \"If the context below is not sufficient or empty, use your own general knowledge to answer the question about this person.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {state['question']}\\nContext: {context}\\nPast Conversation: {state['pastcon']}\"}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    # Ensure the final answer is tagged correctly for the graph to terminate\n",
    "    return {\"answer\": response.content, \"can_answer\": True}\n",
    "\n",
    "\n",
    "def rephrase_question(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Rephrase the question to improve search results.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Rephrase the given question to potentially yield better search results, maintaining the original intent.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Original question: {state['question']}\"}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"question\": response.content}\n",
    "\n",
    "def increment_attempts(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Increment the attempts counter.\n",
    "    \"\"\"\n",
    "    return {\"attempts\": state[\"attempts\"] + 1}\n",
    "\n",
    "def famous_or_not(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Conditional edge to check if the person is famous.\n",
    "    \"\"\"\n",
    "    # If context is found OR the person is famous, we can potentially answer.\n",
    "    if state[\"context\"] or state[\"is_famous\"]:\n",
    "        return \"check_answerable\"\n",
    "    # If no context and not famous, try rephrasing\n",
    "    elif state[\"attempts\"] < 2:\n",
    "        return \"rephrase_question\"\n",
    "    # Exhausted all options\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def should_continue(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Conditional edge to decide the next step after checking answerability.\n",
    "    \"\"\"\n",
    "    # If the LLM confirmed it can answer (either from context or because the person is famous)\n",
    "    if state[\"can_answer\"] or state[\"is_famous\"]:\n",
    "        return \"generate_answer\"\n",
    "    elif state[\"attempts\"] < 2:\n",
    "        return \"rephrase_question\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Define the workflow graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "# RENAMED the node name here\n",
    "workflow.add_node(\"check_if_famous\", check_if_famous)\n",
    "workflow.add_node(\"check_answerable\", check_answerable)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"rephrase_question\", rephrase_question)\n",
    "workflow.add_node(\"increment_attempts\", increment_attempts)\n",
    "\n",
    "# Define the edges\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "# UPDATED the edge to point to the renamed node\n",
    "workflow.add_edge(\"retrieve\", \"check_if_famous\")\n",
    "\n",
    "# UPDATED the conditional edge to start from the renamed node\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_if_famous\",\n",
    "    famous_or_not,\n",
    "    {\n",
    "        \"check_answerable\": \"check_answerable\",\n",
    "        \"rephrase_question\": \"rephrase_question\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_answerable\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"rephrase_question\": \"rephrase_question\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"rephrase_question\", \"increment_attempts\")\n",
    "workflow.add_edge(\"increment_attempts\", \"retrieve\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "nx_graph = graph.networkx_graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Use a layout (e.g., dot for directed graphs, needs pygraphviz or pydot)\n",
    "pos = nx.nx_agraph.graphviz_layout(nx_graph, prog='dot')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "nx.draw(nx_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrows=True)\n",
    "plt.title(\"LangGraph Workflow\")\n",
    "plt.savefig(\"workflow_graph.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_rag_system(did: str, question: str, past_conv: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the RAG system.\n",
    "    \"\"\"\n",
    "    initial_state = {\n",
    "        \"question\": question,\n",
    "        \"context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"attempts\": 0,\n",
    "        \"can_answer\": False,\n",
    "        \"doc_id\": did,\n",
    "        \"pastcon\": past_conv,\n",
    "        \"name\": name,\n",
    "        \"is_famous\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # Check for an answer in the final state; otherwise, provide the default message.\n",
    "    if result.get(\"answer\"):\n",
    "        return result[\"answer\"]\n",
    "    else:\n",
    "        return \"I'm sorry, but I don't have enough information to answer that question.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
